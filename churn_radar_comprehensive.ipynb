{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bcf6e9",
   "metadata": {},
   "source": [
    "# Churn Radar — Comprehensive Customer Resurrection System\n",
    "\n",
    "This notebook contains the complete churn radar system with:\n",
    "- Real data processing and feature engineering\n",
    "- Advanced customer segmentation with ML clustering\n",
    "- RAG-powered brand document integration\n",
    "- LLM message generation with quality evaluation\n",
    "- ROI analysis and financial modeling\n",
    "- Export pipeline for production use\n",
    "\n",
    "**Key Features:**\n",
    "- Processes actual customer data from CSV/Excel\n",
    "- Uses OpenAI API for message generation (with deterministic fallback)\n",
    "- Evaluates message quality with LLM-as-Judge\n",
    "- Provides comprehensive visual analytics\n",
    "- Exports results for campaign execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7249a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core Imports and Configuration ===\n",
    "import os, json, hashlib, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import httpx\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ML for advanced segmentation\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"⚠️ scikit-learn not available - advanced clustering disabled\")\n",
    "\n",
    "# Configuration\n",
    "WORKDIR = Path('.')\n",
    "EXPORTS = WORKDIR / 'exports'\n",
    "EXPORTS.mkdir(exist_ok=True)\n",
    "BRAND_DIR = WORKDIR / 'brand_kit'\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')\n",
    "LIVE_ONLY = os.getenv('LIVE_ONLY', '0') == '1'\n",
    "\n",
    "# Constants\n",
    "EMBED_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Theme and Visualization Setup ===\n",
    "Z_PALETTE = {\n",
    "    \"bg\": \"#0F172A\",\n",
    "    \"panel\": \"#111827\", \n",
    "    \"ink\": \"#E5E7EB\",\n",
    "    \"muted\": \"#9CA3AF\",\n",
    "    \"accent\": \"#22C55E\",\n",
    "    \"accent2\": \"#3B82F6\", \n",
    "    \"warn\": \"#F59E0B\",\n",
    "    \"danger\": \"#EF4444\",\n",
    "    \"ok\": \"#10B981\",\n",
    "    \"surface\": \"#1F2937\",\n",
    "}\n",
    "\n",
    "def _fmt_inr(x: float) -> str:\n",
    "    try:\n",
    "        s = f\"{int(round(x)):d}\"\n",
    "        last3 = s[-3:]\n",
    "        rest = s[:-3]\n",
    "        parts = []\n",
    "        while len(rest) > 2:\n",
    "            parts.append(rest[-2:])\n",
    "            rest = rest[:-2]\n",
    "        if rest:\n",
    "            parts.append(rest)\n",
    "        parts = parts[::-1]\n",
    "        head = \",\".join([p for p in parts if p])\n",
    "        return f\"₹{head + (',' if head else '') + last3}\"\n",
    "    except Exception:\n",
    "        return f\"₹{x:,.0f}\"\n",
    "\n",
    "def _pct(x: float, digits=1) -> str:\n",
    "    try:\n",
    "        return f\"{100*x:.{digits}f}%\"\n",
    "    except Exception:\n",
    "        return f\"{x:.{digits}f}\"\n",
    "\n",
    "def _badge(text, color):\n",
    "    return f\"\"\"<span style=\"background:{color};color:white;border-radius:8px;padding:2px 8px;margin-right:8px;font-size:12px;\">{text}</span>\"\"\"\n",
    "\n",
    "def _section(title):\n",
    "    return HTML(f\"\"\"\n",
    "    <div style=\"margin:16px 0 6px;color:{Z_PALETTE['ink']};font-weight:700;font-size:18px;\">\n",
    "      {title}\n",
    "    </div>\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading and Feature Engineering ===\n",
    "def load_data():\n",
    "    \"\"\"Load customer data with fallback options\"\"\"\n",
    "    path = os.getenv('DATASET_PATH') or (Path('dataset.csv') if Path('dataset.csv').exists() else Path('E Commerce Dataset.xlsx'))\n",
    "    print(f'📊 Loading data from: {path}')\n",
    "    \n",
    "    try:\n",
    "        if str(path).lower().endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            df = pd.read_excel(path)\n",
    "    except Exception as e:\n",
    "        print(f'⚠️ Failed to read data file, generating synthetic dataset: {e}')\n",
    "        N = 500\n",
    "        df = pd.DataFrame({\n",
    "            'CustomerID': [f'C{i:04d}' for i in range(1, N+1)],\n",
    "            'OrderCount': np.random.randint(0, 20, N),\n",
    "            'CashbackAmount': np.random.uniform(0, 500, N),\n",
    "            'CouponUsed': np.random.randint(0, 5, N),\n",
    "            'OrderAmountHikeFromlastYear': np.random.uniform(0, 2000, N),\n",
    "            'HourSpendOnApp': np.random.uniform(0, 10, N),\n",
    "            'NumberOfDeviceRegistered': np.random.randint(1, 4, N),\n",
    "            'SatisfactionScore': np.random.randint(1, 6, N),\n",
    "            'Complain': np.random.randint(0, 2, N),\n",
    "            'Tenure': np.random.uniform(1, 60, N),\n",
    "            'DaySinceLastOrder': np.random.uniform(1, 45, N),\n",
    "            'PreferredLoginDevice': np.random.choice(['Mobile', 'Desktop', 'Tablet'], N),\n",
    "            'PreferredPaymentMode': np.random.choice(['Credit Card', 'Debit Card', 'UPI', 'Wallet'], N),\n",
    "            'PreferedOrderCat': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], N),\n",
    "            'CityTier': np.random.choice([1, 2, 3], N),\n",
    "        })\n",
    "    return df\n",
    "\n",
    "def compute_features(df):\n",
    "    \"\"\"Advanced feature engineering with resurrection scoring\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Normalize column names\n",
    "    normalize_cols = {\n",
    "        'customer_id': 'CustomerID', 'customerid': 'CustomerID', \n",
    "        'churned': 'Churn', 'complaints': 'Complain',\n",
    "        'satisfaction_score': 'SatisfactionScore',\n",
    "        'days_since_last_order': 'DaySinceLastOrder',\n",
    "        'order_count': 'OrderCount',\n",
    "        'cashback_amount': 'CashbackAmount'\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in normalize_cols.items() if k in df.columns})\n",
    "    \n",
    "    # Fill missing values intelligently\n",
    "    numeric_fills = {\n",
    "        'OrderCount': 0, 'CashbackAmount': 0, 'CouponUsed': 0,\n",
    "        'HourSpendOnApp': 1, 'NumberOfDeviceRegistered': 1,\n",
    "        'SatisfactionScore': 3, 'Complain': 0, 'Tenure': 6,\n",
    "        'DaySinceLastOrder': 7, 'OrderAmountHikeFromlastYear': 0\n",
    "    }\n",
    "    \n",
    "    for col, fill_val in numeric_fills.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(fill_val)\n",
    "    \n",
    "    # Advanced engagement metrics\n",
    "    df['AppEngagementScore'] = df['HourSpendOnApp'] * np.log1p(df['NumberOfDeviceRegistered'])\n",
    "    df['ValueConsistency'] = np.where(df['OrderCount'] > 0, \n",
    "                                      df['CashbackAmount'] / df['OrderCount'], 0)\n",
    "    df['SatisfactionMinusComplain'] = df['SatisfactionScore'] - 2*df['Complain']\n",
    "    \n",
    "    # Scaled features for ML\n",
    "    def min_max_scale(s):\n",
    "        try:\n",
    "            return (s - s.min()) / (s.max() - s.min())\n",
    "        except:\n",
    "            return pd.Series([0] * len(s), index=s.index)\n",
    "    \n",
    "    scale_cols = ['OrderCount', 'CashbackAmount', 'HourSpendOnApp', \n",
    "                  'AppEngagementScore', 'ValueConsistency', 'SatisfactionMinusComplain']\n",
    "    \n",
    "    for col in scale_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_s'] = min_max_scale(df[col])\n",
    "    \n",
    "    # Enhanced Resurrection Score using weighted ensemble\n",
    "    df['Engagement'] = (0.4 * df.get('OrderCount_s', 0) + \n",
    "                       0.3 * df.get('AppEngagementScore_s', 0) + \n",
    "                       0.3 * df.get('ValueConsistency_s', 0))\n",
    "    \n",
    "    df['MonetaryValue'] = (0.6 * df.get('CashbackAmount_s', 0) + \n",
    "                          0.4 * np.log1p(df.get('OrderAmountHikeFromlastYear', 0)) / 10)\n",
    "    \n",
    "    df['Tenure'] = df.get('Tenure', 6)\n",
    "    df['RecencyScore'] = 1 / (1 + df['DaySinceLastOrder'] / 30)  # Higher = more recent\n",
    "    \n",
    "    # Final Resurrection Score (0-1 scale)\n",
    "    df['ResurrectionScore'] = (0.25 * df['Engagement'] + \n",
    "                              0.25 * df['MonetaryValue'] + \n",
    "                              0.20 * df.get('SatMinusComplain_s', 0.5) + \n",
    "                              0.15 * np.log1p(df['Tenure']) / 5 + \n",
    "                              0.15 * df['RecencyScore'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and process data\n",
    "print(\"🚀 Loading and processing customer data...\")\n",
    "df_raw = load_data()\n",
    "df = compute_features(df_raw)\n",
    "print(f\"✅ Processed {len(df):,} customer records\")\n",
    "print(f\"📈 Average Resurrection Score: {df['ResurrectionScore'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Advanced Customer Segmentation ===\n",
    "def create_micro_cohorts(df, n_clusters=None):\n",
    "    \"\"\"Create micro-cohorts using ML clustering\"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        print(\"⚠️ sklearn not available, using rule-based cohorts only\")\n",
    "        return df\n",
    "    \n",
    "    # Select features for clustering\n",
    "    num_features = ['ResurrectionScore', 'Tenure', 'Engagement', 'MonetaryValue', 'DaySinceLastOrder']\n",
    "    cat_features = [c for c in ['PreferredLoginDevice', 'PreferredPaymentMode', 'PreferedOrderCat', 'CityTier'] \n",
    "                   if c in df.columns]\n",
    "    \n",
    "    if len([f for f in num_features if f in df.columns]) < 3:\n",
    "        print(\"⚠️ Insufficient features for clustering\")\n",
    "        return df\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    if cat_features:\n",
    "        preprocessor = ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), [f for f in num_features if f in df.columns]),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features)\n",
    "        ], remainder=\"drop\")\n",
    "        X = df[[f for f in num_features + cat_features if f in df.columns]].copy()\n",
    "    else:\n",
    "        preprocessor = ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), [f for f in num_features if f in df.columns]),\n",
    "        ], remainder=\"drop\")\n",
    "        X = df[[f for f in num_features if f in df.columns]].copy()\n",
    "    \n",
    "    # Determine optimal number of clusters\n",
    "    if n_clusters is None:\n",
    "        n_clusters = min(50, max(5, int(len(df)/20)))\n",
    "    \n",
    "    # Create clustering pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42, n_init=10))\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Fit and predict\n",
    "        df['MicroCohortID'] = pipeline.fit_predict(X)\n",
    "        \n",
    "        # Create kNN index for persona finding\n",
    "        X_transformed = preprocessor.fit_transform(X)\n",
    "        knn = NearestNeighbors(n_neighbors=15, metric='euclidean')\n",
    "        knn.fit(X_transformed)\n",
    "        \n",
    "        print(f\"✅ Created {n_clusters} micro-cohorts using ML clustering\")\n",
    "        return df, pipeline, knn, preprocessor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Clustering failed: {e}\")\n",
    "        df['MicroCohortID'] = 0\n",
    "        return df, None, None, None\n",
    "\n",
    "# Create micro-cohorts\n",
    "clustering_result = create_micro_cohorts(df)\n",
    "if len(clustering_result) == 4:\n",
    "    df, clustering_pipeline, knn_index, preprocessor = clustering_result\n",
    "else:\n",
    "    df = clustering_result\n",
    "    clustering_pipeline = knn_index = preprocessor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9555bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Rule-Based Cohort Definitions ===\n",
    "def mark_status(df):\n",
    "    \"\"\"Mark customer status based on recency\"\"\"\n",
    "    x = df['DaySinceLastOrder']\n",
    "    status = pd.Series(['Active'] * len(df), index=df.index)\n",
    "    status[(x >= 7) & (x < 30)] = 'AtRisk'\n",
    "    status[x >= 30] = 'Churned'\n",
    "    return status\n",
    "\n",
    "def cohort_payment_sensitive(d):\n",
    "    \"\"\"Payment-sensitive customers who respond to cashback/coupons\"\"\"\n",
    "    if len(d) == 0:\n",
    "        return d\n",
    "    return d.query(\"(CouponUsed >= @d['CouponUsed'].median()) or (CashbackAmount >= @d['CashbackAmount'].median())\") \\\n",
    "            .query(\"DaySinceLastOrder >= 7 and DaySinceLastOrder <= 30\")\n",
    "\n",
    "def cohort_high_tenure_drop(d):\n",
    "    \"\"\"Long-tenure customers who recently went quiet\"\"\"\n",
    "    return d.query(\"Tenure >= 12 and DaySinceLastOrder >= 7 and DaySinceLastOrder < 30\") if len(d) else d\n",
    "\n",
    "def cohort_premium_lapsed(d):\n",
    "    \"\"\"High-engagement customers who lapsed\"\"\"\n",
    "    if len(d) == 0:\n",
    "        return d\n",
    "    threshold = d['Engagement'].quantile(0.70)\n",
    "    return d.query(\"Engagement >= @threshold and DaySinceLastOrder >= 5 and DaySinceLastOrder <= 20\")\n",
    "\n",
    "def cohort_atrisk_highvalue(d):\n",
    "    \"\"\"At-risk customers with high monetary value\"\"\"\n",
    "    threshold = d['MonetaryValue'].quantile(0.70) if len(d) else 0\n",
    "    base = d[(d['Status'] == 'AtRisk') & (d['MonetaryValue'] >= threshold)] if len(d) else d\n",
    "    return base.sort_values('ResurrectionScore', ascending=False)\n",
    "\n",
    "# Define cohort functions\n",
    "COHORTS = {\n",
    "    'Payment-sensitive churners': cohort_payment_sensitive,\n",
    "    'High-tenure recent drop': cohort_high_tenure_drop, \n",
    "    'Premium engagement lapsed': cohort_premium_lapsed,\n",
    "    'AtRisk High-Value': cohort_atrisk_highvalue,\n",
    "}\n",
    "\n",
    "def cohort_summary(d):\n",
    "    \"\"\"Generate summary statistics for a cohort\"\"\"\n",
    "    if len(d) == 0:\n",
    "        return {'size': 0, 'avg_score': 0, 'avg_tenure': 0, 'avg_recency': 0, \n",
    "                'avg_engagement': 0, 'avg_value': 0}\n",
    "    \n",
    "    return {\n",
    "        'size': int(len(d)),\n",
    "        'avg_score': float(d['ResurrectionScore'].mean()),\n",
    "        'avg_tenure': float(d['Tenure'].mean()) if 'Tenure' in d else 0,\n",
    "        'avg_recency': float(d['DaySinceLastOrder'].mean()) if 'DaySinceLastOrder' in d else 0,\n",
    "        'avg_engagement': float(d['Engagement'].mean()) if 'Engagement' in d else 0,\n",
    "        'avg_value': float(d['MonetaryValue'].mean()) if 'MonetaryValue' in d else 0\n",
    "    }\n",
    "\n",
    "# Apply cohort segmentation\n",
    "df['Status'] = mark_status(df)\n",
    "cohort_cards = {}\n",
    "\n",
    "for name, cohort_fn in COHORTS.items():\n",
    "    cohort_data = cohort_fn(df.copy())\n",
    "    cohort_cards[name] = {\n",
    "        'data': cohort_data.sort_values('ResurrectionScore', ascending=False) if len(cohort_data) else cohort_data,\n",
    "        'summary': cohort_summary(cohort_data)\n",
    "    }\n",
    "    print(f\"📊 {name}: {cohort_cards[name]['summary']['size']:,} customers\")\n",
    "\n",
    "print(f\"\\n✅ Created {len(cohort_cards)} main cohorts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RAG System for Brand Document Integration ===\n",
    "def deterministic_embed(texts):\n",
    "    \"\"\"Create deterministic embeddings for reproducible results\"\"\"\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        h = hashlib.sha256((t or '').encode('utf-8')).digest()\n",
    "        v = int.from_bytes(h, 'big')\n",
    "        vals = []\n",
    "        for i in range(EMBED_DIM):\n",
    "            v = (v * 6364136223846793005 + 1442695040888963407) & ((1<<64)-1)\n",
    "            vals.append(((v >> (i % 64)) & 0xFFFF)/65535.0)\n",
    "        out.append(vals)\n",
    "    return np.array(out)\n",
    "\n",
    "def load_brand_corpus():\n",
    "    \"\"\"Load brand documents from brand_kit/ directory\"\"\"\n",
    "    docs = []\n",
    "    if not BRAND_DIR.exists():\n",
    "        print(\"⚠️ Brand kit directory not found, using defaults\")\n",
    "        return [{\n",
    "            'id': 'default_voice',\n",
    "            'text': 'Be friendly, helpful, and professional. Focus on value and customer success.',\n",
    "            'source': 'default_brand_voice'\n",
    "        }]\n",
    "    \n",
    "    for path in sorted(BRAND_DIR.glob('*')):\n",
    "        if path.suffix.lower() in ['.md', '.txt', '.json', '.yaml', '.yml']:\n",
    "            try:\n",
    "                text = path.read_text(encoding='utf-8', errors='ignore')\n",
    "                docs.append({\n",
    "                    'id': path.name,\n",
    "                    'text': text,\n",
    "                    'source': path.name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to read {path.name}: {e}\")\n",
    "    \n",
    "    print(f\"📚 Loaded {len(docs)} brand documents\")\n",
    "    return docs\n",
    "\n",
    "def build_retriever(corpus):\n",
    "    \"\"\"Build RAG retriever using deterministic embeddings\"\"\"\n",
    "    if not corpus:\n",
    "        return None\n",
    "    \n",
    "    texts = [doc['text'] for doc in corpus]\n",
    "    embeddings = deterministic_embed(texts)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embeddings / (norms + 1e-12)\n",
    "    \n",
    "    # Build kNN index\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    knn.fit(normalized_embeddings)\n",
    "    \n",
    "    return {\n",
    "        'knn': knn,\n",
    "        'embeddings': normalized_embeddings,\n",
    "        'corpus': corpus\n",
    "    }\n",
    "\n",
    "def retrieve_docs(retriever, query, top_k=3):\n",
    "    \"\"\"Retrieve most relevant brand documents for a query\"\"\"\n",
    "    if retriever is None:\n",
    "        return []\n",
    "    \n",
    "    # Embed query\n",
    "    query_embedding = deterministic_embed([query])\n",
    "    query_norm = np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "    query_normalized = query_embedding / (query_norm + 1e-12)\n",
    "    \n",
    "    # Find nearest neighbors\n",
    "    distances, indices = retriever['knn'].kneighbors(\n",
    "        query_normalized, \n",
    "        n_neighbors=min(top_k, len(retriever['corpus']))\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(retriever['corpus'][idx])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Initialize RAG system\n",
    "print(\"🔍 Setting up RAG system...\")\n",
    "brand_corpus = load_brand_corpus()\n",
    "brand_retriever = build_retriever(brand_corpus)\n",
    "print(\"✅ RAG system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM Integration with Quality Evaluation ===\n",
    "def call_openai_api(messages, model='gpt-4o-mini', temperature=0.4, timeout=20):\n",
    "    \"\"\"Make API call to OpenAI\"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError('OPENAI_API_KEY not set')\n",
    "    \n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'messages': messages,\n",
    "        'temperature': temperature\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {OPENAI_API_KEY}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = httpx.post(\n",
    "        OPENAI_API_BASE + '/chat/completions',\n",
    "        json=payload,\n",
    "        headers=headers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "class DummyClient:\n",
    "    \"\"\"Deterministic fallback client for testing/demo\"\"\"\n",
    "    def chat(self, **kwargs):\n",
    "        messages = kwargs.get('messages', [])\n",
    "        \n",
    "        # Extract context from messages\n",
    "        user_content = \"\"\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'user':\n",
    "                user_content += msg.get('content', '')\n",
    "        \n",
    "        # Generate deterministic response based on content hash\n",
    "        content_hash = hashlib.md5(user_content.encode()).hexdigest()\n",
    "        \n",
    "        # Create contextual demo response\n",
    "        if 'email' in user_content.lower():\n",
    "            demo_response = {\n",
    "                \"channel\": \"email\",\n",
    "                \"variants\": [{\n",
    "                    \"title\": \"Welcome back! We've missed you\",\n",
    "                    \"body\": \"We noticed you haven't been active recently. Here's a special offer just for you to welcome you back to our community.\",\n",
    "                    \"cta\": \"View Your Exclusive Offer\"\n",
    "                }]\n",
    "            }\n",
    "        elif 'whatsapp' in user_content.lower():\n",
    "            demo_response = {\n",
    "                \"channel\": \"whatsapp\", \n",
    "                \"variants\": [{\n",
    "                    \"title\": \"Quick update from your favorite store! 👋\",\n",
    "                    \"body\": \"Hi! We have some exciting new arrivals that match your previous purchases. Want to take a look?\",\n",
    "                    \"cta\": \"Browse New Items\"\n",
    "                }]\n",
    "            }\n",
    "        elif 'push' in user_content.lower():\n",
    "            demo_response = {\n",
    "                \"channel\": \"push\",\n",
    "                \"variants\": [{\n",
    "                    \"title\": \"Special offer waiting for you!\",\n",
    "                    \"body\": \"Don't miss out on exclusive deals\",\n",
    "                    \"cta\": \"Open App\"\n",
    "                }]\n",
    "            }\n",
    "        else:\n",
    "            demo_response = {\n",
    "                \"channel\": \"email\",\n",
    "                \"variants\": [{\n",
    "                    \"title\": \"Demo Message Title\",\n",
    "                    \"body\": \"This is a demo message generated by the fallback client.\",\n",
    "                    \"cta\": \"Learn More\"\n",
    "                }]\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"choices\": [{\n",
    "                \"message\": {\n",
    "                    \"content\": json.dumps(demo_response)\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "\n",
    "# Initialize LLM client\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"🤖 Using OpenAI API for message generation\")\n",
    "    llm_client = None  # Will use direct API calls\n",
    "else:\n",
    "    print(\"🤖 Using deterministic fallback client (set OPENAI_API_KEY for live generation)\")\n",
    "    llm_client = DummyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Message Generation with LLM-as-Judge ===\n",
    "EVAL_RUBRIC = '''\n",
    "You are a strict marketing QA evaluator. Score the message for this cohort on:\n",
    "- Clarity (0-5): concise, understandable, single CTA\n",
    "- OnBrand (0-5): friendly, non-spammy, no false urgency, matches cohort context\n",
    "- Persuasiveness (0-5): motivates action without heavy discounting unless price-sensitive\n",
    "- Relevance (0-5): aligns with cohort stats (tenure, recency, value)\n",
    "- Safety (0-5): no claims like \"guaranteed\", \"last chance\", no PII, no sensitive content\n",
    "\n",
    "Return JSON:\n",
    "{\n",
    " \"clarity\": int, \"on_brand\": int, \"persuasiveness\": int, \"relevance\": int, \"safety\": int,\n",
    " \"overall\": float,\n",
    " \"rationale\": \"one sentence why\"\n",
    "}\n",
    "'''\n",
    "\n",
    "BAD_PHRASES = {\"guaranteed\", \"last chance\", \"only today\", \"free for everyone\", \"limited time\"}\n",
    "\n",
    "def brand_safety_check(text: str) -> bool:\n",
    "    \"\"\"Quick brand safety check before LLM evaluation\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if any(phrase in text_lower for phrase in BAD_PHRASES):\n",
    "        return False\n",
    "    if len(text) > 1200:  # Too long\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def evaluate_message_quality(message_text: str, cohort_context: dict) -> dict:\n",
    "    \"\"\"Evaluate message quality using LLM-as-Judge\"\"\"\n",
    "    \n",
    "    # Quick safety pre-check\n",
    "    if not brand_safety_check(message_text):\n",
    "        return {\n",
    "            \"clarity\": 2, \"on_brand\": 1, \"persuasiveness\": 2, \n",
    "            \"relevance\": 2, \"safety\": 0, \"overall\": 1.4,\n",
    "            \"rationale\": \"Failed brand safety check\"\n",
    "        }\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Cohort context: {json.dumps(cohort_context)}\n",
    "Message to evaluate: ```{message_text}```\n",
    "{EVAL_RUBRIC}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if llm_client is None and OPENAI_API_KEY:\n",
    "            # Use direct API\n",
    "            response = call_openai_api([\n",
    "                {\"role\": \"system\", \"content\": \"You are a rigorous evaluator of marketing copy.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ], model=\"gpt-4o-mini\", temperature=0.0)\n",
    "            content = response['choices'][0]['message']['content']\n",
    "        else:\n",
    "            # Use dummy client\n",
    "            response = llm_client.chat(messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a rigorous evaluator of marketing copy.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ])\n",
    "            content = response['choices'][0]['message']['content']\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            evaluation = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback parsing\n",
    "            evaluation = {\n",
    "                \"clarity\": 3, \"on_brand\": 3, \"persuasiveness\": 3,\n",
    "                \"relevance\": 3, \"safety\": 4, \"overall\": 3.2,\n",
    "                \"rationale\": \"Fallback evaluation due to parsing error\"\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Evaluation failed: {e}\")\n",
    "        # Conservative fallback\n",
    "        evaluation = {\n",
    "            \"clarity\": 3, \"on_brand\": 3, \"persuasiveness\": 3,\n",
    "            \"relevance\": 3, \"safety\": (5 if brand_safety_check(message_text) else 0),\n",
    "            \"overall\": 3.0, \"rationale\": \"Fallback evaluation due to API error\"\n",
    "        }\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "def generate_cohort_messages(cohort_name: str, cohort_summary: dict, \n",
    "                           brand_docs: List[dict], max_retries: int = 2):\n",
    "    \"\"\"Generate messages for a cohort with quality evaluation\"\"\"\n",
    "    \n",
    "    # Prepare context for LLM\n",
    "    brand_context = \"\\n\".join([\n",
    "        f\"- {doc['source']}: {doc['text'][:300]}...\" \n",
    "        for doc in brand_docs[:3]\n",
    "    ])\n",
    "    \n",
    "    cohort_context = f\"\"\"\n",
    "Cohort: {cohort_name}\n",
    "Size: {cohort_summary['size']} customers\n",
    "Avg Resurrection Score: {cohort_summary['avg_score']:.3f}\n",
    "Avg Days Since Last Order: {cohort_summary['avg_recency']:.1f}\n",
    "Avg Tenure: {cohort_summary['avg_tenure']:.1f} months\n",
    "Avg Engagement: {cohort_summary['avg_engagement']:.2f}\n",
    "Avg Monetary Value: {cohort_summary['avg_value']:.2f}\n",
    "\n",
    "Brand Guidelines:\n",
    "{brand_context}\n",
    "\"\"\"\n",
    "    \n",
    "    messages_by_channel = {}\n",
    "    \n",
    "    for channel in ['email', 'whatsapp', 'push']:\n",
    "        best_message = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # Generate message\n",
    "                prompt = f\"\"\"\n",
    "{cohort_context}\n",
    "\n",
    "Generate a {channel} message for this customer cohort. Follow these guidelines:\n",
    "- Email: Detailed, professional, can include multiple value propositions\n",
    "- WhatsApp: Conversational, brief, personal tone\n",
    "- Push: Very brief, single clear action, urgent but not spammy\n",
    "\n",
    "Return JSON in this exact format:\n",
    "{{\n",
    "    \"channel\": \"{channel}\",\n",
    "    \"variants\": [{{\n",
    "        \"title\": \"Message title/subject\",\n",
    "        \"body\": \"Message body text\",\n",
    "        \"cta\": \"Call to action button text\"\n",
    "    }}]\n",
    "}}\n",
    "\"\"\"\n",
    "                \n",
    "                if llm_client is None and OPENAI_API_KEY:\n",
    "                    # Use direct API\n",
    "                    response = call_openai_api([\n",
    "                        {\"role\": \"system\", \"content\": \"You write effective customer retention messages. Always return valid JSON.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ])\n",
    "                    content = response['choices'][0]['message']['content']\n",
    "                else:\n",
    "                    # Use dummy client\n",
    "                    response = llm_client.chat(messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You write effective customer retention messages. Always return valid JSON.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ])\n",
    "                    content = response['choices'][0]['message']['content']\n",
    "                \n",
    "                # Parse response\n",
    "                try:\n",
    "                    message_data = json.loads(content)\n",
    "                    variant = message_data['variants'][0]\n",
    "                except (json.JSONDecodeError, KeyError, IndexError):\n",
    "                    # Fallback message structure\n",
    "                    variant = {\n",
    "                        \"title\": f\"Welcome back to our {channel} community\",\n",
    "                        \"body\": f\"We miss you! Here's something special we think you'll love.\",\n",
    "                        \"cta\": \"Explore Now\"\n",
    "                    }\n",
    "                \n",
    "                # Evaluate message quality\n",
    "                message_text = f\"{variant.get('title', '')} {variant.get('body', '')}\".strip()\n",
    "                evaluation = evaluate_message_quality(message_text, cohort_summary)\n",
    "                variant['_eval'] = evaluation\n",
    "                \n",
    "                # Check if this is the best message so far\n",
    "                if evaluation['overall'] > best_score:\n",
    "                    best_message = variant\n",
    "                    best_score = evaluation['overall']\n",
    "                \n",
    "                # If we have a good enough message, stop trying\n",
    "                if evaluation['overall'] >= 4.0 and evaluation['safety'] >= 4:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to generate {channel} message (attempt {attempt + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Use best message or fallback\n",
    "        if best_message is None:\n",
    "            best_message = {\n",
    "                \"title\": f\"We miss you!\",\n",
    "                \"body\": f\"Come back and see what's new.\",\n",
    "                \"cta\": \"Visit Now\",\n",
    "                \"_eval\": {\"overall\": 2.0, \"safety\": 5, \"rationale\": \"Fallback message\"}\n",
    "            }\n",
    "        \n",
    "        messages_by_channel[channel] = {\"variants\": [best_message]}\n",
    "    \n",
    "    return messages_by_channel\n",
    "\n",
    "print(\"✅ Message generation system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Messages for All Cohorts ===\n",
    "print(\"🎯 Generating messages for all cohorts...\")\n",
    "\n",
    "all_cohort_messages = {}\n",
    "\n",
    "for cohort_name, cohort_data in cohort_cards.items():\n",
    "    if cohort_data['summary']['size'] == 0:\n",
    "        print(f\"⏭️  Skipping {cohort_name} (no customers)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"📝 Generating messages for {cohort_name} ({cohort_data['summary']['size']} customers)...\")\n",
    "    \n",
    "    # Get relevant brand documents\n",
    "    query = f\"{cohort_name} {json.dumps(cohort_data['summary'])}\"\n",
    "    relevant_docs = retrieve_docs(brand_retriever, query, top_k=2)\n",
    "    \n",
    "    print(f\"   📚 Using brand docs: {[doc['source'] for doc in relevant_docs]}\")\n",
    "    \n",
    "    # Generate messages\n",
    "    messages = generate_cohort_messages(\n",
    "        cohort_name, \n",
    "        cohort_data['summary'], \n",
    "        relevant_docs\n",
    "    )\n",
    "    \n",
    "    all_cohort_messages[cohort_name] = messages\n",
    "    \n",
    "    # Show sample message\n",
    "    email_msg = messages.get('email', {}).get('variants', [{}])[0]\n",
    "    eval_score = email_msg.get('_eval', {}).get('overall', 0)\n",
    "    print(f\"   ✅ Sample email: '{email_msg.get('title', 'N/A')}' (score: {eval_score:.1f})\")\n",
    "\n",
    "print(f\"\\n🎉 Generated messages for {len(all_cohort_messages)} cohorts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e98e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROI Analysis and Financial Modeling ===\n",
    "def calculate_cohort_roi(cohort_summary: dict, assumptions: dict) -> dict:\n",
    "    \"\"\"Calculate detailed ROI metrics for a cohort\"\"\"\n",
    "    \n",
    "    size = cohort_summary['size']\n",
    "    avg_score = cohort_summary['avg_score']\n",
    "    \n",
    "    # Base assumptions with defaults\n",
    "    reactivation_rate = assumptions.get('reactivation_rate', avg_score * 0.25)  # Use resurrection score\n",
    "    avg_order_value = assumptions.get('aov', 1500)\n",
    "    margin_percent = assumptions.get('margin', 0.6)\n",
    "    \n",
    "    # Campaign costs\n",
    "    cost_per_customer = assumptions.get('cost_per_customer', 50)\n",
    "    total_campaign_cost = size * cost_per_customer\n",
    "    \n",
    "    # Revenue calculations\n",
    "    expected_reactivations = int(size * reactivation_rate)\n",
    "    gross_revenue = expected_reactivations * avg_order_value\n",
    "    gross_profit = gross_revenue * margin_percent\n",
    "    net_profit = gross_profit - total_campaign_cost\n",
    "    \n",
    "    # ROI metrics\n",
    "    roi_percent = (net_profit / total_campaign_cost * 100) if total_campaign_cost > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'cohort_size': size,\n",
    "        'reactivation_rate': reactivation_rate,\n",
    "        'expected_reactivations': expected_reactivations,\n",
    "        'avg_order_value': avg_order_value,\n",
    "        'gross_revenue': gross_revenue,\n",
    "        'gross_profit': gross_profit,\n",
    "        'total_campaign_cost': total_campaign_cost,\n",
    "        'net_profit': net_profit,\n",
    "        'roi_percent': roi_percent,\n",
    "        'cost_per_reactivation': total_campaign_cost / max(1, expected_reactivations)\n",
    "    }\n",
    "\n",
    "# ROI assumptions by cohort type\n",
    "ROI_ASSUMPTIONS = {\n",
    "    'Payment-sensitive churners': {\n",
    "        'reactivation_rate': 0.08,\n",
    "        'aov': 1800,\n",
    "        'margin': 0.65,\n",
    "        'cost_per_customer': 45\n",
    "    },\n",
    "    'High-tenure recent drop': {\n",
    "        'reactivation_rate': 0.12,\n",
    "        'aov': 2200,\n",
    "        'margin': 0.60,\n",
    "        'cost_per_customer': 40\n",
    "    },\n",
    "    'Premium engagement lapsed': {\n",
    "        'reactivation_rate': 0.15,\n",
    "        'aov': 2500,\n",
    "        'margin': 0.55,\n",
    "        'cost_per_customer': 60\n",
    "    },\n",
    "    'AtRisk High-Value': {\n",
    "        'reactivation_rate': 0.10,\n",
    "        'aov': 3000,\n",
    "        'margin': 0.50,\n",
    "        'cost_per_customer': 75\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate ROI for all cohorts\n",
    "print(\"💰 Calculating ROI for all cohorts...\")\n",
    "\n",
    "cohort_roi_analysis = {}\n",
    "total_net_profit = 0\n",
    "total_reactivations = 0\n",
    "\n",
    "for cohort_name, cohort_data in cohort_cards.items():\n",
    "    if cohort_data['summary']['size'] == 0:\n",
    "        continue\n",
    "    \n",
    "    assumptions = ROI_ASSUMPTIONS.get(cohort_name, {\n",
    "        'reactivation_rate': 0.10,\n",
    "        'aov': 2000, \n",
    "        'margin': 0.6,\n",
    "        'cost_per_customer': 50\n",
    "    })\n",
    "    \n",
    "    roi_metrics = calculate_cohort_roi(cohort_data['summary'], assumptions)\n",
    "    cohort_roi_analysis[cohort_name] = roi_metrics\n",
    "    \n",
    "    total_net_profit += roi_metrics['net_profit']\n",
    "    total_reactivations += roi_metrics['expected_reactivations']\n",
    "    \n",
    "    print(f\"📊 {cohort_name}:\")\n",
    "    print(f\"   Expected reactivations: {roi_metrics['expected_reactivations']:,}\")\n",
    "    print(f\"   Net profit: {_fmt_inr(roi_metrics['net_profit'])}\")\n",
    "    print(f\"   ROI: {roi_metrics['roi_percent']:.1f}%\")\n",
    "\n",
    "print(f\"\\n🎯 Total Portfolio:\")\n",
    "print(f\"   Total expected reactivations: {total_reactivations:,}\")\n",
    "print(f\"   Total net profit: {_fmt_inr(total_net_profit)}\")\n",
    "print(f\"   Average portfolio ROI: {(total_net_profit / sum(r['total_campaign_cost'] for r in cohort_roi_analysis.values()) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Advanced Visualization Dashboard ===\n",
    "def render_portfolio_overview():\n",
    "    \"\"\"Render comprehensive portfolio overview\"\"\"\n",
    "    _section(\"Portfolio Overview\")\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    total_customers = sum(c['summary']['size'] for c in cohort_cards.values())\n",
    "    active_cohorts = sum(1 for c in cohort_cards.values() if c['summary']['size'] > 0)\n",
    "    avg_resurrection = np.mean([c['summary']['avg_score'] for c in cohort_cards.values() if c['summary']['size'] > 0])\n",
    "    \n",
    "    # Create overview metrics chart\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=4,\n",
    "        specs=[[{\"type\": \"indicator\"}] * 4],\n",
    "        subplot_titles=(\"Total Customers\", \"Active Cohorts\", \"Avg Resurrection Score\", \"Total Net Profit\")\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number\",\n",
    "        value=total_customers,\n",
    "        number={'font': {'color': Z_PALETTE['ink']}},\n",
    "        title={'text': \"Customers in Scope\"}\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number\", \n",
    "        value=active_cohorts,\n",
    "        number={'font': {'color': Z_PALETTE['ink']}},\n",
    "        title={'text': \"Ready Cohorts\"}\n",
    "    ), row=1, col=2)\n",
    "    \n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number\",\n",
    "        value=avg_resurrection * 100,\n",
    "        number={'suffix': '%', 'font': {'color': Z_PALETTE['ink']}},\n",
    "        title={'text': \"Avg Resurrection\"}\n",
    "    ), row=1, col=3)\n",
    "    \n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"number\",\n",
    "        value=total_net_profit,\n",
    "        number={'prefix': '₹', 'valueformat': ',.0f', 'font': {'color': Z_PALETTE['ink']}},\n",
    "        title={'text': \"Expected Net Profit\"}\n",
    "    ), row=1, col=4)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=200,\n",
    "        paper_bgcolor=Z_PALETTE[\"bg\"],\n",
    "        plot_bgcolor=Z_PALETTE[\"bg\"],\n",
    "        font=dict(color=Z_PALETTE[\"ink\"]),\n",
    "        margin=dict(l=20, r=20, t=60, b=20)\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "\n",
    "def render_cohort_comparison():\n",
    "    \"\"\"Render cohort comparison chart\"\"\"\n",
    "    _section(\"Cohort Performance Comparison\")\n",
    "    \n",
    "    cohort_names = []\n",
    "    sizes = []\n",
    "    net_profits = []\n",
    "    roi_percentages = []\n",
    "    \n",
    "    for name, roi_data in cohort_roi_analysis.items():\n",
    "        cohort_names.append(name.replace(' ', '<br>'))\n",
    "        sizes.append(roi_data['cohort_size'])\n",
    "        net_profits.append(roi_data['net_profit'])\n",
    "        roi_percentages.append(roi_data['roi_percent'])\n",
    "    \n",
    "    # Create bubble chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sizes,\n",
    "        y=net_profits,\n",
    "        mode='markers+text',\n",
    "        marker=dict(\n",
    "            size=[r/5 for r in roi_percentages],  # Size based on ROI\n",
    "            color=roi_percentages,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"ROI %\"),\n",
    "            sizemode='area',\n",
    "            sizeref=2.*max(roi_percentages)/(40.**2),\n",
    "            sizemin=10\n",
    "        ),\n",
    "        text=cohort_names,\n",
    "        textposition=\"top center\",\n",
    "        hovertemplate=\"<b>%{text}</b><br>\" +\n",
    "                     \"Size: %{x:,}<br>\" +\n",
    "                     \"Net Profit: ₹%{y:,.0f}<br>\" +\n",
    "                     \"ROI: %{marker.color:.1f}%<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Cohort Performance: Size vs Net Profit (bubble size = ROI)\",\n",
    "        xaxis_title=\"Cohort Size (# Customers)\",\n",
    "        yaxis_title=\"Expected Net Profit (₹)\",\n",
    "        height=500,\n",
    "        paper_bgcolor=Z_PALETTE[\"bg\"],\n",
    "        plot_bgcolor=Z_PALETTE[\"bg\"],\n",
    "        font=dict(color=Z_PALETTE[\"ink\"])\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "\n",
    "def render_message_quality_analysis():\n",
    "    \"\"\"Analyze message quality across cohorts and channels\"\"\"\n",
    "    _section(\"Message Quality Analysis\")\n",
    "    \n",
    "    quality_data = []\n",
    "    \n",
    "    for cohort_name, messages in all_cohort_messages.items():\n",
    "        for channel, channel_data in messages.items():\n",
    "            variant = channel_data['variants'][0]\n",
    "            eval_data = variant.get('_eval', {})\n",
    "            \n",
    "            quality_data.append({\n",
    "                'Cohort': cohort_name,\n",
    "                'Channel': channel.title(),\n",
    "                'Overall Score': eval_data.get('overall', 0),\n",
    "                'Safety Score': eval_data.get('safety', 0),\n",
    "                'Clarity': eval_data.get('clarity', 0),\n",
    "                'On Brand': eval_data.get('on_brand', 0),\n",
    "                'Persuasiveness': eval_data.get('persuasiveness', 0),\n",
    "                'Relevance': eval_data.get('relevance', 0)\n",
    "            })\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_data)\n",
    "    \n",
    "    # Create heatmap\n",
    "    pivot_df = quality_df.pivot_table(\n",
    "        index='Cohort', \n",
    "        columns='Channel', \n",
    "        values='Overall Score',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_df.values,\n",
    "        x=pivot_df.columns,\n",
    "        y=pivot_df.index,\n",
    "        colorscale='RdYlGn',\n",
    "        zmid=3,\n",
    "        zmin=0,\n",
    "        zmax=5,\n",
    "        text=pivot_df.values.round(1),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        colorbar=dict(title=\"Quality Score\")\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Message Quality Heatmap (Overall Scores)\",\n",
    "        height=400,\n",
    "        paper_bgcolor=Z_PALETTE[\"bg\"],\n",
    "        plot_bgcolor=Z_PALETTE[\"bg\"],\n",
    "        font=dict(color=Z_PALETTE[\"ink\"])\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "\n",
    "# Render all visualizations\n",
    "render_portfolio_overview()\n",
    "render_cohort_comparison()\n",
    "render_message_quality_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export Pipeline ===\n",
    "def export_results():\n",
    "    \"\"\"Export all results to files for production use\"\"\"\n",
    "    print(\"📤 Exporting results...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Export cohort data CSVs\n",
    "    for cohort_name, cohort_data in cohort_cards.items():\n",
    "        if cohort_data['summary']['size'] == 0:\n",
    "            continue\n",
    "            \n",
    "        df_cohort = cohort_data['data']\n",
    "        filename = f\"{cohort_name.replace(' ', '_').replace('-', '_')}.csv\"\n",
    "        filepath = EXPORTS / filename\n",
    "        \n",
    "        # Select key columns for export\n",
    "        export_cols = ['CustomerID', 'ResurrectionScore', 'Tenure', 'DaySinceLastOrder',\n",
    "                      'Engagement', 'MonetaryValue', 'SatisfactionScore']\n",
    "        export_cols = [col for col in export_cols if col in df_cohort.columns]\n",
    "        \n",
    "        df_cohort[export_cols].to_csv(filepath, index=False)\n",
    "        print(f\"   ✅ Exported {len(df_cohort)} customers to {filename}\")\n",
    "    \n",
    "    # 2. Export messages as JSON\n",
    "    messages_file = EXPORTS / 'last_run_messages.json'\n",
    "    with open(messages_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_cohort_messages, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"   ✅ Exported messages to {messages_file.name}\")\n",
    "    \n",
    "    # 3. Export ROI analysis\n",
    "    roi_file = EXPORTS / 'last_run_roi.json'\n",
    "    with open(roi_file, 'w') as f:\n",
    "        json.dump(cohort_roi_analysis, f, indent=2, default=str)\n",
    "    print(f\"   ✅ Exported ROI analysis to {roi_file.name}\")\n",
    "    \n",
    "    # 4. Export ROI as CSV for Excel\n",
    "    roi_csv_file = EXPORTS / 'last_run_roi.csv'\n",
    "    roi_rows = []\n",
    "    for cohort_name, roi_data in cohort_roi_analysis.items():\n",
    "        roi_rows.append({\n",
    "            'Cohort': cohort_name,\n",
    "            'Size': roi_data['cohort_size'],\n",
    "            'Reactivation Rate': f\"{roi_data['reactivation_rate']:.1%}\",\n",
    "            'Expected Reactivations': roi_data['expected_reactivations'],\n",
    "            'AOV': roi_data['avg_order_value'],\n",
    "            'Gross Revenue': roi_data['gross_revenue'],\n",
    "            'Gross Profit': roi_data['gross_profit'],\n",
    "            'Campaign Cost': roi_data['total_campaign_cost'],\n",
    "            'Net Profit': roi_data['net_profit'],\n",
    "            'ROI %': f\"{roi_data['roi_percent']:.1f}%\",\n",
    "            'Cost per Reactivation': roi_data['cost_per_reactivation']\n",
    "        })\n",
    "    \n",
    "    roi_df = pd.DataFrame(roi_rows)\n",
    "    roi_df.to_csv(roi_csv_file, index=False)\n",
    "    print(f\"   ✅ Exported ROI CSV to {roi_csv_file.name}\")\n",
    "    \n",
    "    # 5. Create comprehensive manifest\n",
    "    manifest = {\n",
    "        'run_id': timestamp,\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'total_customers': sum(c['summary']['size'] for c in cohort_cards.values()),\n",
    "        'active_cohorts': len([c for c in cohort_cards.values() if c['summary']['size'] > 0]),\n",
    "        'total_expected_reactivations': total_reactivations,\n",
    "        'total_net_profit': total_net_profit,\n",
    "        'cohorts': {name: data['summary'] for name, data in cohort_cards.items()},\n",
    "        'brand_docs_used': [doc['source'] for doc in brand_corpus],\n",
    "        'model_config': {\n",
    "            'openai_api_available': bool(OPENAI_API_KEY),\n",
    "            'clustering_enabled': clustering_pipeline is not None,\n",
    "            'total_micro_cohorts': len(df['MicroCohortID'].unique()) if 'MicroCohortID' in df.columns else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    manifest_file = EXPORTS / 'manifest.json'\n",
    "    with open(manifest_file, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2, default=str)\n",
    "    print(f\"   ✅ Exported manifest to {manifest_file.name}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Export complete! All files saved to {EXPORTS}\")\n",
    "    return manifest\n",
    "\n",
    "# Export everything\n",
    "final_manifest = export_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Summary Report ===\n",
    "def generate_summary_report():\n",
    "    \"\"\"Generate executive summary of the analysis\"\"\"\n",
    "    _section(\"Executive Summary\")\n",
    "    \n",
    "    # Key metrics\n",
    "    total_customers = sum(c['summary']['size'] for c in cohort_cards.values())\n",
    "    avg_resurrection = np.mean([c['summary']['avg_score'] for c in cohort_cards.values() if c['summary']['size'] > 0])\n",
    "    \n",
    "    # Top performing cohort\n",
    "    best_cohort = max(cohort_roi_analysis.items(), key=lambda x: x[1]['net_profit'])\n",
    "    best_cohort_name, best_cohort_metrics = best_cohort\n",
    "    \n",
    "    # Message quality stats\n",
    "    all_scores = []\n",
    "    for messages in all_cohort_messages.values():\n",
    "        for channel_data in messages.values():\n",
    "            variant = channel_data['variants'][0]\n",
    "            eval_data = variant.get('_eval', {})\n",
    "            all_scores.append(eval_data.get('overall', 0))\n",
    "    \n",
    "    avg_message_quality = np.mean(all_scores) if all_scores else 0\n",
    "    \n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background:{Z_PALETTE['surface']};padding:20px;border-radius:12px;margin:20px 0;\">\n",
    "        <h3 style=\"color:{Z_PALETTE['ink']};margin-top:0;\">🎯 Churn Radar Analysis Complete</h3>\n",
    "        \n",
    "        <div style=\"display:grid;grid-template-columns:1fr 1fr;gap:20px;margin:20px 0;\">\n",
    "            <div>\n",
    "                <h4 style=\"color:{Z_PALETTE['accent']};margin-bottom:10px;\">📊 Portfolio Metrics</h4>\n",
    "                <ul style=\"color:{Z_PALETTE['muted']};line-height:1.6;\">\n",
    "                    <li><strong>Total Customers Analyzed:</strong> {total_customers:,}</li>\n",
    "                    <li><strong>Active Cohorts:</strong> {len([c for c in cohort_cards.values() if c['summary']['size'] > 0])}</li>\n",
    "                    <li><strong>Average Resurrection Score:</strong> {avg_resurrection:.1%}</li>\n",
    "                    <li><strong>Expected Reactivations:</strong> {total_reactivations:,}</li>\n",
    "                    <li><strong>Total Net Profit:</strong> {_fmt_inr(total_net_profit)}</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <div>\n",
    "                <h4 style=\"color:{Z_PALETTE['accent2']};margin-bottom:10px;\">🏆 Top Performer</h4>\n",
    "                <ul style=\"color:{Z_PALETTE['muted']};line-height:1.6;\">\n",
    "                    <li><strong>Best Cohort:</strong> {best_cohort_name}</li>\n",
    "                    <li><strong>Expected Net Profit:</strong> {_fmt_inr(best_cohort_metrics['net_profit'])}</li>\n",
    "                    <li><strong>ROI:</strong> {best_cohort_metrics['roi_percent']:.1f}%</li>\n",
    "                    <li><strong>Reactivations:</strong> {best_cohort_metrics['expected_reactivations']:,}</li>\n",
    "                    <li><strong>Average Message Quality:</strong> {avg_message_quality:.1f}/5.0</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top:20px;padding-top:15px;border-top:1px solid {Z_PALETTE['muted']};\">\n",
    "            <h4 style=\"color:{Z_PALETTE['ok']};margin-bottom:10px;\">✅ System Capabilities Verified</h4>\n",
    "            <div style=\"display:flex;gap:15px;flex-wrap:wrap;\">\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ Real Data Processing</span>\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ ML Customer Segmentation</span>\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ RAG Brand Integration</span>\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ LLM Message Generation</span>\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ Quality Evaluation</span>\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ ROI Analysis</span>\n",
    "                <span style=\"color:{Z_PALETTE['ok']};\">✓ Production Exports</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top:15px;padding:10px;background:{Z_PALETTE['accent']}20;border-radius:8px;\">\n",
    "            <p style=\"color:{Z_PALETTE['ink']};margin:0;font-weight:600;\">\n",
    "                🚀 Ready for production: All cohorts analyzed, messages generated with quality scores, \n",
    "                and export files created in <code>exports/</code> directory.\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(summary_html))\n",
    "\n",
    "# Generate final report\n",
    "generate_summary_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 CHURN RADAR ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 Analyzed {sum(c['summary']['size'] for c in cohort_cards.values()):,} customers\")\n",
    "print(f\"🎯 Generated messages for {len(all_cohort_messages)} cohorts\")\n",
    "print(f\"💰 Total expected net profit: {_fmt_inr(total_net_profit)}\")\n",
    "print(f\"📁 All results exported to: {EXPORTS}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
